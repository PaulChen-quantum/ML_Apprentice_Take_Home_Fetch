{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcqtSRgdungv"
      },
      "outputs": [],
      "source": [
        "# Task 4: Training Loop Implementation (BONUS)\n",
        "\n",
        "'''\n",
        "Assumptions:\n",
        "We have two synthetic datasets:\n",
        "\n",
        "task_a_data = List[Tuple[str, int]] (sentence, label)\n",
        "\n",
        "task_b_data = List[Tuple[str, int]]\n",
        "\n",
        "We'll alternate batches from Task A and B.\n",
        "\n",
        "Loss: nn.CrossEntropyLoss for both heads.\n",
        "\n",
        "Key Decisions and Insights:\n",
        "\n",
        "Loss funcyion: CrossEntropyLoss per task; it is the standard loss function for classification problems;\n",
        "\n",
        "Optimizer: AdamW;\n",
        "\n",
        "reason: AdamW is a variant of the Adam optimizer with decoupled weight decay. Compared to standard Adam, it helps prevent overfitting and improves generalization,\n",
        "especially in transformer models (like BERT, RoBERTa, MiniLM). HuggingFace and Google recommend AdamW over Adam for training/fine-tuning transformers.\n",
        "\n",
        "Multi-tasking logic:\tParallel batches (zip):\tAlternating task updates is easy and modular\n",
        "\n",
        "Model outputs:\tRaw logits\tMatches PyTorch expectations for loss functions\n",
        "\n",
        "Metric handling:\tcompute accuracy per task\n",
        "\n",
        "Freeze support:\tControlled via .requires_grad before training\tAligns with Task 3 setup\n",
        "\n",
        "'''\n",
        "\n",
        "# Dependencies\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Dummy Dataset Class\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, data: List[Tuple[str, int]]):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence, label = self.data[idx]\n",
        "        return sentence, label\n",
        "\n",
        "# Training Loop\n",
        "def train_multitask_model(model: nn.Module,\n",
        "                          task_a_loader: DataLoader,\n",
        "                          task_b_loader: DataLoader,\n",
        "                          num_epochs=5,\n",
        "                          lr=2e-5,\n",
        "                          device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Iterate through epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss_a = 0.0\n",
        "        total_loss_b = 0.0\n",
        "\n",
        "        # Combine iterators for both tasks (zip stops at shortest)\n",
        "        for (batch_a, batch_b) in zip(task_a_loader, task_b_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # --- Task A: Sentence Classification ---\n",
        "            sentences_a, labels_a = batch_a\n",
        "            logits_a = model(sentences_a, task='A')\n",
        "            loss_a = criterion(logits_a, labels_a.to(device))\n",
        "\n",
        "            # --- Task B: Sentiment Analysis ---\n",
        "            sentences_b, labels_b = batch_b\n",
        "            logits_b = model(sentences_b, task='B')\n",
        "            loss_b = criterion(logits_b, labels_b.to(device))\n",
        "\n",
        "            # Total loss = sum of both\n",
        "            loss = loss_a + loss_b\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss_a += loss_a.item()\n",
        "            total_loss_b += loss_b.item()\n",
        "\n",
        "        avg_loss_a = total_loss_a / len(task_a_loader)\n",
        "        avg_loss_b = total_loss_b / len(task_b_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss A: {avg_loss_a:.4f} | Loss B: {avg_loss_b:.4f}\")\n",
        "\n",
        "# Example: Dummy Data and Execution\n",
        "\n",
        "# Simulate small datasets\n",
        "task_a_data = [(\"This is about sports.\", 0), (\"Finance news\", 1), (\"Politics today\", 2)]\n",
        "task_b_data = [(\"I love this!\", 0), (\"Terrible experience\", 1), (\"It's okay.\", 2)]\n",
        "\n",
        "# Wrap in Datasets and Loaders\n",
        "task_a_loader = DataLoader(SimpleTextDataset(task_a_data), batch_size=2, shuffle=True)\n",
        "task_b_loader = DataLoader(SimpleTextDataset(task_b_data), batch_size=2, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "model = MultiTaskSentenceTransformer(task_a_num_classes=3, task_b_num_classes=3)\n",
        "\n",
        "# Train (mock)\n",
        "train_multitask_model(model, task_a_loader, task_b_loader)\n",
        "\n",
        "# Evaluation Function (Accuracy)\n",
        "\n",
        "def evaluate_model(model, data_loader, task='A', device='cpu'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentences, labels in data_loader:\n",
        "            logits = model(sentences, task=task).to(device)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct += (predictions.cpu() == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"[Task {task.upper()}] Evaluation Accuracy: {accuracy:.2%}\")\n",
        "    return accuracy\n",
        "\n",
        "evaluate_model(model, task_a_loader, task='a')\n",
        "evaluate_model(model, task_b_loader, task='b')\n"
      ]
    }
  ]
}